# thoughts

todo
- science, utilitarianism and the missing gap of objective levels of happiness and suffering
- longtermism - if we assume there is a minimal chance x of everything dying per year, then we should use `(1-x)^t` as a discount factor in our utility function, exactly how it is done in reinforcement learning. Therefore, future utility is likely not as vast.
- utilitarianism: is the ethics with the least arbitrary assumptions - how would you act if you would experience the universe from the perspective of everyone and everything one after the other
