# When an AI would convince me that it is self-aware
![A man made out of water](img/emerging-sentience.png =400x400)
Discussions with LLMs like the lambda interview are no convincing evidence of actual self-awareness - I believe it is impossible to judge from observation or behavior if a system is conscious. With a lot of time, someone could make a database of all reasonable responses to written conversations of at most 1000000 characters length and have intelligent conversations with us about its own existence and place in the universe, but it would all just be a database lookup.

What *would* convince me that a ML model is self-aware and even has qualia would be if it would be trained in whatever way, but without training data that contains any concepts related to consciousness, self-awareness or emotions, but comes up with them on its own.

This is only a way to prove that a system has these properties, but we still have no way of proving that a system does not have these properties.