# The basics

- The fundamental questions
    - how to find out what is true? -> epistemics
    - what is true? -> some facts about the universe
    - how to find out what is good? -> introspection, seeking coherence, seeking few bits
    - what is good? -> utilitarianism
    - how to maximize what is good?

- Epistemics
    - directly accessible information: different states of mind
    - coherence of states of mind lead to a model of an external world that follows certain rules
    - how to pick one of many models coherent with observations? bayes. what to use as prior: ??? ~ ocam razor

- Facts about the universe
    - object permance
    - time exists and is the direction of causality
    - gravity exists
    - light exists
    - entropy increases
    - most likely the universe started with a big bang and is expanding
        - elements exist, light exists, elements have characteristic colors, colors are wavelength, we see movement relative to ourselves by subtle changes of the color of the light of stars, ...
    - the universe started 13.6B years ago, earth exists ~4.5B years, life exists ~2B years, humans exist ~200k years
    - ...
    - interesting open questions
        - is the universe discrete?
            yes: we might live in a simulation. the state transition function 
            no: unclear if computational models exist that make the universe a computable function

- How to know what is good?
    - one can think of arbitrary objectives. how to decide what is good?
    - we can start with minimal assumptions and see what the coherent expansion leads to.
    - minimal assumptions I use:
        - being happy is better than not being happy. I experience this first hand, even before I experience that an outer universe exists
        - any value system must be coherent. the question "is world A better than world B" should have a definite answer that depends only on world A and B, it cannot have different answers that are equally valid

- What is good
    - certain states of mind are inherently better than others. Goodness of mental states exists on a scale.
    - minimal assumptions that lead to utilitarianism
        - Total order: for two different worlds A and B, either A is better than B, B is better than A, or they are equally good.
            - in other words: no two worlds are inherently uncomparable
                - what if this assumption is wrong? A ? B -> A' ~ B where A~ is A with a lot of extra suffering. seems unplausible
                  steal man: there exist at least two different inherently valuable things that cannot be weighted against each other, e.g. happiness and beauty of the world.
                  then an action that adds the tiniest amount of beauty but decreases happiness by as much as physically possible would not be bad but undefined. Similarly, an action that makes the world a little bit more ugly but adds as much happiness as possible would not be good. Almost every action has many chaotic effects and makes the world either more or less beautiful and more or less happy, and whenever the direction of change in these dimensions is not the same, we have no grounds to call that action good or bad. Since we also don't know the chaotic effects of actions we dont know which actions can be said to be good or bad, and we loose all judgement of the world.

            - game theory tells us: a preference relation can be expressed by a utility function
        - Aggregationism: discovering a new planet that we cannot caussally effect cannot change what is good on this planet.
    
    - this concludes in utilitarianism being true, but not necessarily informative
        - utility can be unknowable (just like P = NP ? is def either true or false but we might never know)

    - utilitarianism and the hard problem of consciousness
        - we know that happiness exists because we directly experience it
        - we observe other humans and animals that are like us and therefore believe they also experience happiness

    - intuitive arguments
        - complete empathy: how would you want to act if you would experience the universe from the perspective of every consciouss being?
        - value is the integral of the happiness curve
    
    - constructing a utility scale
        - to get utility of event E, ask: "for what P(E) would I be indifferent between:
            - option 1: i lose (gain) one average day of my life, E happens with P(E)
            - option 2: i dont lose a day of my life, E does not happen
        the utility of E is 1/P(E) (or if it's negative: -1/P(E))
    
    - questions that are unclear to me
        - hard problem of sentience levels (ofc)
        - imagine three brains: A, B, C. A is a happy human, B is a happy human that runs really slow, lives 1M years but experiences it like we do 80 years, brain C experiences at our pace but intensity of feelings is much lower than ours. All experience the same amount of happiness during their lifes. How can this be expressed in the integral over time framework? If you ask B they rate their happiness at any moment much higher than C

- how to maximize what is good
    - extreme cases
        - worst possible universe: all energy is used for suffering
        - best possible universe: all energy is used to directly create happy qualia
        - both worlds would in practise look like this: every star has a dyson sphere with structures that experience good or bad qualia and probably not much would happen over time, it would be something like a meditative state rather than a world with people living eventful lives
    - right now:
        - there is an enormous amount of animals suffering at every moment. we need to stop it
            - at every moment, for each person ~
        - money has decreasing marginal utility, therefore transfering money from rich to poor people would greatly improve the world
        - to the degree to which we can estimate the consequences of our actions, the future might be incredibly important.
            - best kinf of possible trajectory from here:
                - end factory farming
                - avoid extinction and get all extinction risks to ~0 (climate change, bad AGI, pandemics)
                - remove reasons for suffering: hunger, disease, violence, pain, mental health issues
                - improve wellbeing: research better drugs, direct brain stimulation, etc
                - populate the universe with happy beings (simulated or living)
            - worst kind of possible trajectory from here:
                - let the world continue to exist similar to as it is now
                    why is it so bad? -> factory farming
  